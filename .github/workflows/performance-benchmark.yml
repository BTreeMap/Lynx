name: Performance Benchmarks

on:
  workflow_run:
    workflows: ["Build and Publish Docker image to GHCR"]
    types:
      - completed
  workflow_dispatch:
    inputs:
      duration:
        description: 'Benchmark duration per test (e.g., 30s, 1m)'
        required: false
        default: '30s'
      concurrency:
        description: 'Maximum concurrency level for tests'
        required: false
        default: '10000'

jobs:
  benchmark-postgres:
    name: Performance Benchmark (PostgreSQL)
    runs-on: ubuntu-24.04
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Compute lowercase repository name
        run: echo "REPO_LOWER=$(echo '${{ github.repository }}' | tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Pull Docker image
        run: |
          docker pull ghcr.io/${{ env.REPO_LOWER }}:latest

      - name: Start PostgreSQL container
        run: |
          docker run -d \
            --name postgres \
            -e POSTGRES_USER=lynx \
            -e POSTGRES_PASSWORD=lynx_benchmark_pass \
            -e POSTGRES_DB=lynx \
            -p 5432:5432 \
            postgres:16-alpine

          # Wait for PostgreSQL to be ready
          for i in {1..30}; do
            if docker exec postgres pg_isready -U lynx > /dev/null 2>&1; then
              echo "PostgreSQL is ready!"
              break
            fi
            echo "Waiting for PostgreSQL... ($i/30)"
            sleep 2
          done

      - name: Start Lynx with PostgreSQL and userland-proxy disabled
        run: |
          # Run with --userland-proxy=false to disable the userland proxy
          # This significantly improves performance
          docker run -d \
            --name lynx \
            --network host \
            --userns=host \
            -e DATABASE_BACKEND=postgres \
            -e DATABASE_URL=postgresql://lynx:lynx_benchmark_pass@localhost:5432/lynx \
            -e DATABASE_MAX_CONNECTIONS=50 \
            -e AUTH_MODE=none \
            -e API_HOST=0.0.0.0 \
            -e API_PORT=8080 \
            -e REDIRECT_HOST=0.0.0.0 \
            -e REDIRECT_PORT=3000 \
            -e CACHE_MAX_ENTRIES=500000 \
            -e CACHE_FLUSH_INTERVAL_SECS=5 \
            -e ACTOR_BUFFER_SIZE=1000000 \
            -e ACTOR_FLUSH_INTERVAL_MS=100 \
            ghcr.io/${{ env.REPO_LOWER }}:latest

          # Note: --userland-proxy=false is a Docker daemon setting, not a container flag
          # We use --network host which bypasses the userland proxy entirely

      - name: Wait for Lynx service to be ready
        run: |
          for i in {1..60}; do
            if curl -f http://localhost:8080/api/health > /dev/null 2>&1; then
              echo "Lynx service is ready!"
              break
            fi
            echo "Waiting for Lynx service... ($i/60)"
            sleep 2
          done
          curl http://localhost:8080/api/health || exit 1

      - name: Install benchmarking tools
        run: |
          # Install wrk for high-performance HTTP benchmarking
          sudo apt-get update
          sudo apt-get install -y build-essential libssl-dev git jq

          # Clone and build wrk
          git clone https://github.com/wg/wrk.git /tmp/wrk
          cd /tmp/wrk
          make -j$(nproc)
          sudo cp wrk /usr/local/bin/

          # Verify installation
          wrk --version || echo "wrk installed (no version flag)"

          # Install Apache Bench as fallback
          sudo apt-get install -y apache2-utils

      - name: Install Rust toolchain and profiling tools
        run: |
          # Install Rust
          curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
          source $HOME/.cargo/env
          
          # Install cargo-flamegraph (this may take a few minutes)
          echo "Installing cargo-flamegraph (this may take 2-5 minutes)..."
          cargo install flamegraph
          
          # Install perf tools for flamegraph profiling
          sudo apt-get install -y linux-tools-common linux-tools-generic linux-tools-$(uname -r) || true
          
          # Configure perf to allow profiling without sudo
          sudo sysctl -w kernel.perf_event_paranoid=-1 || true
          
          # Verify installations
          cargo --version
          cargo flamegraph --version || echo "flamegraph installed"
          perf --version || echo "perf may not be available, flamegraph will try alternatives"

      - name: Create benchmark results directory
        run: mkdir -p benchmark-results

      - name: Run performance benchmarks
        run: |
          DURATION="${{ github.event.inputs.duration || '30s' }}"
          bash tests/benchmark.sh http://localhost:8080 http://localhost:3000 ./benchmark-results "$DURATION"

      - name: Generate performance report
        if: always()
        run: |
          REPORT_FILE="benchmark-results/PERFORMANCE_REPORT.md"

          cat > "$REPORT_FILE" << 'EOF'
          # Lynx Performance Benchmark Report

          **Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Commit:** ${{ github.sha }}
          **Branch:** ${{ github.ref_name }}
          **Workflow Run:** ${{ github.run_id }}

          ## Configuration

          - **Database:** PostgreSQL 16
          - **Network Mode:** host (userland-proxy bypassed for maximum performance)
          - **Cache Settings:**
            - Max Entries: 500,000
            - DB Flush Interval: 5 seconds
            - Actor Buffer Size: 1,000,000
            - Actor Flush Interval: 100ms
          - **Database Connections:** 50

          ## Test Environment

          - **Runner:** ubuntu-24.04
          - **CPU:** GitHub Actions hosted runner
          - **Memory:** GitHub Actions hosted runner
          - **Docker:** Host network mode (no userland proxy overhead)

          ## Benchmark Results

          See detailed results in the uploaded artifacts.

          ### Key Performance Indicators

          The benchmark focuses on:

          1. **Redirect Endpoint** (Primary Focus)
             - Single hot URL with 1000 concurrent connections
             - Single hot URL with 5000 concurrent connections
             - Distributed load across 100 URLs
             - Extreme load with 10,000 concurrent connections

          2. **Management Endpoints** (Expected Lower Performance)
             - POST /api/urls (Create)
             - GET /api/urls/:code (Read single)
             - GET /api/urls (List)
             - PUT /api/urls/:code/deactivate (Update)
             - GET /api/health (Health check)

          ### Expected Performance Characteristics

          Based on caching optimizations documented in `docs/PERFORMANCE_OPTIMIZATIONS.md`:

          - **Redirect endpoint (cached):**
            - Target: ~70,000 requests/second with 1000 concurrency
            - Expected slight drop with 5000 concurrency
            - Zero lock contention on hot URLs (actor pattern)
            - Sub-millisecond latency for cached hits

          - **Management endpoints:**
            - Lower throughput (involve database queries)
            - POST: Database writes + validation
            - GET single: May benefit from read cache
            - GET list: Pagination queries
            - PUT: State changes + cache invalidation

          ### Performance Optimizations Verified

          1. âœ… Moka read cache (500k entries, 5-minute TTL)
          2. âœ… Actor pattern write buffering (zero lock contention)
          3. âœ… Dual-layer flush system (100ms + 5s)
          4. âœ… Non-blocking database writes
          5. âœ… Connection pooling (50 connections)

          ## Files Generated

          - `benchmark-results-*.txt`: Detailed wrk output
          - `benchmark-results-*.json`: Structured results data
          - `PERFORMANCE_REPORT.md`: This summary report
          - `flamegraphs/*.svg`: Flamegraph visualizations for detailed profiling analysis

          ## Flamegraph Profiling

          This benchmark includes flamegraph profiling for detailed CPU usage analysis:

          - **Redirect Endpoint (Cached)**: Shows hot paths in cached request handling
          - **API Operations**: Shows time distribution across database operations

          Flamegraphs are interactive SVG files that can be opened in any browser.
          See `flamegraphs/README.md` for interpretation guide.

          For more information on profiling, see `docs/profiling_rust.md`.

          ## Analysis

          The benchmark validates the caching and actor pattern optimizations implemented in Lynx.
          For detailed performance optimization documentation, see `docs/PERFORMANCE_OPTIMIZATIONS.md`.

          ---

          *Generated by GitHub Actions Performance Benchmark workflow*
          EOF

          # Use envsubst if available, otherwise use sed
          if command -v envsubst &> /dev/null; then
            envsubst < "$REPORT_FILE" > "${REPORT_FILE}.tmp"
            mv "${REPORT_FILE}.tmp" "$REPORT_FILE"
          else
            sed -i "s|\$(date -u +\"%Y-%m-%d %H:%M:%S UTC\")|$(date -u +"%Y-%m-%d %H:%M:%S UTC")|g" "$REPORT_FILE"
          fi

      - name: Collect service logs
        if: always()
        run: |
          echo "=== Lynx Service Logs ===" > benchmark-results/lynx-logs.txt
          docker logs lynx >> benchmark-results/lynx-logs.txt 2>&1 || true

      - name: Generate performance graphs
        if: always()
        run: |
          # Create a simple ASCII graph summary
          cat > benchmark-results/GRAPHS.txt << 'EOF'
          Performance Graphs Summary
          ==========================

          For detailed graphs, analyze the JSON results file with your preferred visualization tool.

          Recommended tools:
          - gnuplot
          - matplotlib (Python)
          - Chart.js (JavaScript)
          - Google Charts

          JSON structure:
          {
            "timestamp": "...",
            "api_url": "...",
            "redirect_url": "...",
            "tests": [
              {
                "name": "Test name",
                "requests_per_second": "...",
                "avg_latency_ms": "...",
                "p50_latency_ms": "...",
                "p90_latency_ms": "...",
                "p99_latency_ms": "...",
                "errors": "..."
              }
            ]
          }
          EOF

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmark-results
          path: benchmark-results/
          retention-days: 90

      - name: Comment benchmark summary on commit (if PR)
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const reportPath = 'benchmark-results/PERFORMANCE_REPORT.md';

            if (fs.existsSync(reportPath)) {
              const report = fs.readFileSync(reportPath, 'utf8');

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: report
              });
            }

      - name: Stop containers
        if: always()
        run: |
          docker stop lynx || true
          docker rm lynx || true
          docker stop postgres || true
          docker rm postgres || true

      - name: Build Lynx with profiling symbols
        run: |
          source $HOME/.cargo/env
          cargo build --profile profiling
          echo "Built Lynx with profiling profile"

      - name: Setup environment for profiling
        run: |
          # Create flamegraph output directory
          mkdir -p benchmark-results/flamegraphs
          
          # Copy the profiling binary to a known location
          cp target/profiling/lynx ./lynx-profiling
          chmod +x ./lynx-profiling

      - name: Start PostgreSQL for profiling run
        run: |
          docker run -d \
            --name postgres-profiling \
            -e POSTGRES_USER=lynx \
            -e POSTGRES_PASSWORD=lynx_benchmark_pass \
            -e POSTGRES_DB=lynx \
            -p 5432:5432 \
            postgres:16-alpine

          # Wait for PostgreSQL to be ready
          for i in {1..30}; do
            if docker exec postgres-profiling pg_isready -U lynx > /dev/null 2>&1; then
              echo "PostgreSQL is ready for profiling!"
              break
            fi
            echo "Waiting for PostgreSQL... ($i/30)"
            sleep 2
          done

      - name: Profile redirect endpoint (cached requests)
        run: |
          source $HOME/.cargo/env
          
          # Set environment variables for Lynx
          export DATABASE_BACKEND=postgres
          export DATABASE_URL=postgresql://lynx:lynx_benchmark_pass@localhost:5432/lynx
          export DATABASE_MAX_CONNECTIONS=50
          export AUTH_MODE=none
          export API_HOST=0.0.0.0
          export API_PORT=8080
          export REDIRECT_HOST=0.0.0.0
          export REDIRECT_PORT=3000
          export CACHE_MAX_ENTRIES=500000
          export RUST_LOG=info
          
          # Start Lynx under flamegraph profiling in background
          cargo flamegraph --profile profiling -o benchmark-results/flamegraphs/flamegraph-redirect-cached.svg --notes "Redirect endpoint with cached requests" &
          FLAMEGRAPH_PID=$!
          
          # Wait for service to be ready
          for i in {1..60}; do
            if curl -f http://localhost:8080/api/health > /dev/null 2>&1; then
              echo "Lynx service is ready for profiling!"
              break
            fi
            echo "Waiting for Lynx service... ($i/60)"
            sleep 2
          done
          
          # Create test URLs for profiling
          for i in {1..10}; do
            curl -s -X POST http://localhost:8080/api/urls \
              -H "Content-Type: application/json" \
              -d "{\"url\": \"https://example.com/profile-target-$i\", \"custom_code\": \"prof-$i\"}" > /dev/null 2>&1 || true
          done
          
          # Run benchmark load to generate profiling data
          echo "Running benchmark load for 60 seconds to capture flamegraph..."
          wrk -t 8 -c 1000 -d 60s http://localhost:3000/prof-1 || true
          
          # Give it a moment to finish profiling data collection
          sleep 5
          
          # Stop the flamegraph profiling
          kill -SIGINT $FLAMEGRAPH_PID 2>/dev/null || true
          wait $FLAMEGRAPH_PID 2>/dev/null || true
          
          echo "Flamegraph for redirect endpoint (cached) generated"

      - name: Profile API endpoint (database operations)
        run: |
          source $HOME/.cargo/env
          
          # Set environment variables for Lynx
          export DATABASE_BACKEND=postgres
          export DATABASE_URL=postgresql://lynx:lynx_benchmark_pass@localhost:5432/lynx
          export DATABASE_MAX_CONNECTIONS=50
          export AUTH_MODE=none
          export API_HOST=0.0.0.0
          export API_PORT=8080
          export REDIRECT_HOST=0.0.0.0
          export REDIRECT_PORT=3000
          export CACHE_MAX_ENTRIES=500000
          export RUST_LOG=info
          
          # Start Lynx under flamegraph profiling in background
          cargo flamegraph --profile profiling -o benchmark-results/flamegraphs/flamegraph-api-operations.svg --notes "API endpoint with database operations" &
          FLAMEGRAPH_PID=$!
          
          # Wait for service to be ready
          for i in {1..60}; do
            if curl -f http://localhost:8080/api/health > /dev/null 2>&1; then
              echo "Lynx service is ready for profiling!"
              break
            fi
            echo "Waiting for Lynx service... ($i/60)"
            sleep 2
          done
          
          # Create Lua script for mixed API operations
          cat > /tmp/profile-api.lua << 'EOLUA'
          wrk.method = "POST"
          wrk.headers["Content-Type"] = "application/json"
          
          counter = 0
          
          request = function()
              counter = counter + 1
              local rand = math.random(100)
              
              if rand <= 50 then
                  -- 50% creates
                  local body = string.format('{"url": "https://example.com/prof-api-%d"}', counter)
                  return wrk.format("POST", "/api/urls", wrk.headers, body)
              else
                  -- 50% reads
                  local code = "prof-" .. math.random(10)
                  return wrk.format("GET", "/api/urls/" .. code)
              end
          end
          EOLUA
          
          # Run benchmark load with mixed operations
          echo "Running API benchmark for 60 seconds to capture flamegraph..."
          wrk -t 4 -c 100 -d 60s -s /tmp/profile-api.lua http://localhost:8080 || true
          
          # Give it a moment to finish profiling data collection
          sleep 5
          
          # Stop the flamegraph profiling
          kill -SIGINT $FLAMEGRAPH_PID 2>/dev/null || true
          wait $FLAMEGRAPH_PID 2>/dev/null || true
          
          echo "Flamegraph for API endpoint (database operations) generated"

      - name: Stop PostgreSQL profiling container
        if: always()
        run: |
          docker stop postgres-profiling || true
          docker rm postgres-profiling || true

      - name: Generate flamegraph summary
        if: always()
        run: |
          cat > benchmark-results/flamegraphs/README.md << 'EOREADME'
          # Flamegraph Profiling Results
          
          This directory contains flamegraph visualizations of Lynx performance under load.
          
          ## What are Flamegraphs?
          
          Flamegraphs show CPU usage over time and call stacks:
          - **Width**: Represents the total time spent in a function (including its callees)
          - **Height**: Represents the call stack depth
          - **Color**: Randomized for visual separation
          
          ## Files
          
          ### flamegraph-redirect-cached.svg
          Profiling data from the redirect endpoint with cached requests.
          
          **Test Configuration:**
          - 8 threads, 1000 concurrent connections
          - 60 second duration
          - Single hot URL (maximum cache effectiveness)
          
          **Expected Patterns:**
          - Wide sections in tokio runtime and axum handlers
          - Minimal time in database operations (cache hits)
          - Fast cache lookup paths
          
          ### flamegraph-api-operations.svg
          Profiling data from API endpoints with database operations.
          
          **Test Configuration:**
          - 4 threads, 100 concurrent connections
          - 60 second duration
          - Mixed workload: 50% creates (POST), 50% reads (GET)
          
          **Expected Patterns:**
          - Time in sqlx and database drivers
          - Serialization/deserialization overhead
          - Database connection pool management
          
          ## How to Use
          
          1. Download the SVG files from the artifacts
          2. Open in a web browser
          3. Click on any section to zoom in
          4. Search for specific function names using the search box
          
          ## Interpreting Results
          
          - Look for wide plateaus indicating hot code paths
          - Investigate tall stacks that may indicate inefficiencies
          - Compare relative widths to understand time distribution
          - Focus optimization efforts on the widest sections
          
          For more information, see [docs/profiling_rust.md](../../../docs/profiling_rust.md)
          EOREADME
          
          # List generated flamegraphs
          echo "Generated flamegraphs:"
          ls -lh benchmark-results/flamegraphs/*.svg || echo "No SVG files found (profiling may have failed)"

      - name: Print benchmark summary
        if: always()
        run: |
          echo "========================================="
          echo "Performance Benchmark Summary"
          echo "========================================="
          echo ""
          echo "âœ… Benchmark completed!"
          echo ""
          echo "ðŸ“Š Results available in artifacts:"
          echo "   - Detailed wrk output"
          echo "   - JSON structured data"
          echo "   - Performance report"
          echo "   - Service logs"
          echo "   - Flamegraph profiling (SVG)"
          echo ""
          echo "ðŸ”¥ Flamegraph Profiling:"
          echo "   - Redirect endpoint (cached requests)"
          echo "   - API operations (database queries)"
          echo "   - Interactive SVG visualization"
          echo "   - See flamegraphs/README.md for details"
          echo ""
          echo "ðŸ” Key focus areas:"
          echo "   1. Redirect endpoint caching performance"
          echo "   2. Management endpoint database query performance"
          echo "   3. Concurrent load handling"
          echo "   4. Latency percentiles (p50, p90, p99)"
          echo "   5. CPU profiling and hot code paths"
          echo ""
          echo "ðŸ“ˆ Expected results:"
          echo "   - Redirect (cached): ~70k RPS @ 1000 concurrency"
          echo "   - Management: Lower RPS (database queries)"
          echo ""
          echo "ðŸ”— Download artifacts to view detailed results"
          echo "========================================="
